{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPAM or HAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mantunes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/mantunes/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/mantunes/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mantunes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import nltk\n",
    "import tqdm\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 15]\n",
    "plt.rcParams['figure.dpi'] = 72\n",
    "import seaborn as sns\n",
    "import seaborn.objects as so\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import gensim\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score, f1_score\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \"is_categorical_dtype\")\n",
    "warnings.filterwarnings(\"ignore\", \"use_inf_as_na\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5_521, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Target</th><th>SMS</th></tr><tr><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;ham&quot;</td><td>&quot;Go until juron…</td></tr><tr><td>&quot;ham&quot;</td><td>&quot;Ok lar... Joki…</td></tr><tr><td>&quot;spam&quot;</td><td>&quot;Free entry in …</td></tr><tr><td>&quot;ham&quot;</td><td>&quot;U dun say so e…</td></tr><tr><td>&quot;ham&quot;</td><td>&quot;Nah I don&#x27;t th…</td></tr><tr><td>&quot;spam&quot;</td><td>&quot;FreeMsg Hey th…</td></tr><tr><td>&quot;ham&quot;</td><td>&quot;Even my brothe…</td></tr><tr><td>&quot;ham&quot;</td><td>&quot;As per your re…</td></tr><tr><td>&quot;spam&quot;</td><td>&quot;WINNER!! As a …</td></tr><tr><td>&quot;spam&quot;</td><td>&quot;Had your mobil…</td></tr><tr><td>&quot;ham&quot;</td><td>&quot;I&#x27;m gonna be h…</td></tr><tr><td>&quot;spam&quot;</td><td>&quot;SIX chances to…</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;ham&quot;</td><td>&quot;Anything lor. …</td></tr><tr><td>&quot;ham&quot;</td><td>&quot;Get me out of …</td></tr><tr><td>&quot;ham&quot;</td><td>&quot;Ok lor... Sony…</td></tr><tr><td>&quot;ham&quot;</td><td>&quot;Ard 6 like dat…</td></tr><tr><td>&quot;ham&quot;</td><td>&quot;Why don&#x27;t you …</td></tr><tr><td>&quot;ham&quot;</td><td>&quot;Huh y lei...&quot;</td></tr><tr><td>&quot;spam&quot;</td><td>&quot;REMINDER FROM …</td></tr><tr><td>&quot;spam&quot;</td><td>&quot;This is the 2n…</td></tr><tr><td>&quot;ham&quot;</td><td>&quot;Will �_ b goin…</td></tr><tr><td>&quot;ham&quot;</td><td>&quot;Pity, * was in…</td></tr><tr><td>&quot;ham&quot;</td><td>&quot;The guy did so…</td></tr><tr><td>&quot;ham&quot;</td><td>&quot;Rofl. Its true…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5_521, 2)\n",
       "┌────────┬───────────────────────────────────┐\n",
       "│ Target ┆ SMS                               │\n",
       "│ ---    ┆ ---                               │\n",
       "│ str    ┆ str                               │\n",
       "╞════════╪═══════════════════════════════════╡\n",
       "│ ham    ┆ Go until jurong point, crazy.. A… │\n",
       "│ ham    ┆ Ok lar... Joking wif u oni...     │\n",
       "│ spam   ┆ Free entry in 2 a wkly comp to w… │\n",
       "│ ham    ┆ U dun say so early hor... U c al… │\n",
       "│ …      ┆ …                                 │\n",
       "│ ham    ┆ Will �_ b going to esplanade fr … │\n",
       "│ ham    ┆ Pity, * was in mood for that. So… │\n",
       "│ ham    ┆ The guy did some bitching but I … │\n",
       "│ ham    ┆ Rofl. Its true to its name        │\n",
       "└────────┴───────────────────────────────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.read_csv('../../datasets/spam.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.rows()\n",
    "corpus = [text for _, text in dataset]\n",
    "y = [label for label,_ in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "tokens = [nltk.word_tokenize(sample) for sample in corpus]\n",
    "tokens_clean = [[lemmatizer.lemmatize(w).lower() for w in t if len(lemmatizer.lemmatize(w)) > 2 and w.isalpha() and w not in stop_words] for t in tokens]\n",
    "tokens_merge = [' '.join(line) for line in tokens_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data : 4416\n",
      "Testing Data  : 1105\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tokens_merge, y, stratify=y, test_size=0.2, random_state=42)\n",
    "print(f'Training Data : {len(X_train)}')\n",
    "print(f'Testing Data  : {len(X_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data CV : (4416, 256)\n",
      "Test Data CV : (1105, 256)\n"
     ]
    }
   ],
   "source": [
    "# train Bag of Words model\n",
    "cv = CountVectorizer(ngram_range = (1, 2), max_features=256)\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "print(f'Training Data CV : {X_train_cv.shape}')\n",
    "\n",
    "# transform X_test using CV\n",
    "X_test_cv = cv.transform(X_test)\n",
    "print(f'Test Data CV : {X_test_cv.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR  0.96 0.96 0.83\n",
      "KNN 0.95 0.95 0.79\n",
      "NB  0.50 0.57 0.28\n",
      "RFC 0.96 0.96 0.84\n",
      "MLP 0.97 0.97 0.88\n"
     ]
    }
   ],
   "source": [
    "# define the list of classifiers\n",
    "clfs = [\n",
    "    ('LR', LogisticRegression(random_state=42, multi_class='auto', max_iter=600)),\n",
    "    ('KNN', KNeighborsClassifier(n_neighbors=1)),\n",
    "    ('NB', GaussianNB()),\n",
    "    ('RFC', RandomForestClassifier(random_state=42)),\n",
    "    ('MLP', MLPClassifier(random_state=42, learning_rate='adaptive', max_iter=1000))\n",
    "]\n",
    "\n",
    "# whenever possible used joblib to speed-up the training\n",
    "with joblib.parallel_backend('loky', n_jobs=-1):\n",
    "    for label, clf in clfs:\n",
    "        # train the model\n",
    "        clf.fit(X_train_cv.toarray(), y_train)\n",
    "\n",
    "        # generate predictions\n",
    "        predictions = clf.predict(X_test_cv.toarray())\n",
    "\n",
    "        # compute the performance metrics\n",
    "        mcc = matthews_corrcoef(y_test, predictions)\n",
    "        acc = accuracy_score(y_test, predictions)\n",
    "        f1 = f1_score(y_test, predictions, average='weighted')\n",
    "        print(f'{label:3} {acc:.2f} {f1:.2f} {mcc:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
